# -*- coding: utf-8 -*-
"""LogisticRegression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lLuu2fHchsjWQO1FkNOlv1nESPXXIlgw
"""

# Classification in Machine Learning: An Introduction - https://www.datacamp.com/blog/classification-machine-learning
# HIGHRISK should def have corellation with Stroke

# Commented out IPython magic to ensure Python compatibility.
# %pip install seaborn
# %pip install imblearn
from sklearn.model_selection import train_test_split
from imblearn.under_sampling import RandomUnderSampler
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import scipy.stats as stats
from scipy.stats import norm,chisquare
from sklearn.feature_selection import mutual_info_classif
import seaborn as sns
from sklearn.feature_selection import chi2
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_selection import SelectKBest
from sklearn.linear_model import LogisticRegression
from sklearn import metrics
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import cross_validate
from sklearn.model_selection import KFold

# necessary sklearn junk for logistic regression taken from geeks for heeks
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

def LogPredict(df1):
  df = df1

  mi_counts = df['MI_EVENT'].value_counts()
  mortality_counts = df['MORTALITY_EVENT'].value_counts()
  stroke_counts = df['STROKE_EVENT'].value_counts()

  #print("MI Event Counts:\n", mi_counts)
  #print("\nMortality Event Counts:\n", mortality_counts)
  #print("\nStroke Event Counts:\n", stroke_counts)
  # Loads more healthy patients
  def barMI():
    # Plotting MI_EVENT
    mi_counts.plot(kind='bar', title='MI Event Distribution')
    plt.xlabel('MI_EVENT')
    plt.ylabel('Count')
    plt.show()
  def barMort():
    # Plotting MORTALITY_EVENT
    mortality_counts.plot(kind='bar', title='Mortality Event Distribution')
    plt.xlabel('MORTALITY_EVENT')
    plt.ylabel('Count')
    plt.show()
  def barStroke():
    # Plotting STROKE_EVENT
    stroke_counts.plot(kind='bar', title='Stroke Event Distribution')
    plt.xlabel('STROKE_EVENT')
    plt.ylabel('Count')
    plt.show()
    # Results: IMBALANCED DATA
  def pieStroke():
    pieY = np.array(df['STROKE_EVENT'].value_counts())
    plt.pie(pieY, autopct='%0.3f%%', labels=['No Stroke', 'Stroke'])
    plt.legend(labels=['No Stroke: ' + str(df['STROKE_EVENT'].value_counts()[0]), 'Stroke: ' + str(df['STROKE_EVENT'].value_counts()[1])], loc='lower right')
    plt.savefig('stroke.png', dpi = 300)
    plt.show()

  def pieMort():
    pieY = np.array(df['MORTALITY_EVENT'].value_counts())
    plt.pie(pieY, autopct='%0.3f%%', labels=['No Mortality', 'Mortality'])
    plt.legend(labels=['No Mortality: ' + str(df['MORTALITY_EVENT'].value_counts()[0]), 'Mortality: ' + str(df['MORTALITY_EVENT'].value_counts()[1])], loc='lower right')
    plt.savefig('mortality.png', dpi = 300)
    plt.show()

  def pieMI():
    pieY = np.array(df['MI_EVENT'].value_counts())
    plt.pie(pieY, autopct='%0.3f%%', labels=['No Heart Attack', 'Heart Attack'])
    plt.legend(labels=['No Heart Attack: ' + str(df['MI_EVENT'].value_counts()[0]), 'Heart Attack: ' + str(df['MI_EVENT'].value_counts()[1])], loc='lower right')
    plt.savefig('MI.png', dpi = 300)
    plt.show()
    df['MI_EVENT'].value_counts()

  #cell just used for testing and messing around with matplotlib pi chart IGNORE
  #pieY = np.array(df['MORTALITY_EVENT'].value_counts())
  #plt.pie(pieY, autopct='%0.3f%%', labels=['No Mortality', 'Mortality'])
  #plt.legend(labels=['No Mortality: ' + str(df['MORTALITY_EVENT'].value_counts()[0]), 'Mortality: ' + str(df['MORTALITY_EVENT'].value_counts()[1])])
  #plt.show()


  #test endings here

  mi_ratio = mi_counts.min() / mi_counts.max()
  mortality_ratio = mortality_counts.min() / mortality_counts.max()
  stroke_ratio = stroke_counts.min() / stroke_counts.max()

  # Data def needs resampling (random undersampling, SMOTE oversampling)
  # Option of using Cost-sensitive Logistic Regression.

  columns_to_exclude = ['MORTALITY_EVENT', 'MI_EVENT', 'STROKE_EVENT', 'Unnamed: 0', 'PROCEDUREID']
  X = df.drop(columns = columns_to_exclude)
  mortality_y = df['MORTALITY_EVENT']
  mi_y = df['MI_EVENT']
  stroke_y = df['STROKE_EVENT']

  # Using Random Undersampling to balance data
  rus = RandomUnderSampler(sampling_strategy=1)

  X_res_mi, y_res_mi = rus.fit_resample(X, mi_y)
  X_res_mortality, y_res_mortality = rus.fit_resample(X, mortality_y)
  X_res_stroke, y_res_stroke = rus.fit_resample(X, stroke_y)


  # MI_EVENT undersampling
  ax = y_res_mi.value_counts().plot.pie(autopct='%.2f')
  _ = ax.set_title("Under-sampling")

  # Mortality undersampling
  ax = y_res_mortality.value_counts().plot.pie(autopct='%.2f')
  _ = ax.set_title("Under-sampling")

  # Stroke undersampling
  ax = y_res_stroke.value_counts().plot.pie(autopct='%.2f')
  _ = ax.set_title("Under-sampling")

  chi_mortality = chi2(X_res_mortality, y_res_mortality)
  chi_mi = chi2(X_res_mi, y_res_mi)
  chi_stroke = chi2(X_res_stroke, y_res_stroke)

  # Example scores and p-values from your chi-squared test
  # probably wouldve been easier to make a function lol
  mortality_scores = chi_mortality[0]
  mortality_p_values = chi_mortality[1]

  mi_scores = chi_mi[0]
  mi_p_values = chi_mi[1]

  stroke_scores = chi_stroke[0]
  stroke_p_values = chi_stroke[1]


  # Create a DataFrame for better visualization
  mortality_features_df = pd.DataFrame({'Feature': X_res_mortality.columns, 'Score': mortality_scores, 'p-value': mortality_p_values})
  mi_features_df = pd.DataFrame({'Feature': X_res_mi.columns, 'Score': mi_scores, 'p-value': mi_p_values})
  stroke_features_df = pd.DataFrame({'Feature': X_res_stroke.columns, 'Score': stroke_scores, 'p-value': stroke_p_values})

  # Set significance level
  alpha = 0.05

  # Filter for significant features
  mortality_significant_features = mortality_features_df[mortality_features_df['p-value'] < alpha]
  mi_significant_features = mi_features_df[mi_features_df['p-value'] < alpha]
  stroke_significant_features = stroke_features_df[stroke_features_df['p-value'] < alpha]

  # Optionally sort by score
  mortality_significant_features_sorted = mortality_significant_features.sort_values(by='Score', ascending=False)
  mi_significant_features_sorted = mi_significant_features.sort_values(by='Score', ascending=False)
  stroke_significant_features_sorted = stroke_significant_features.sort_values(by='Score', ascending=False)



  def predict():
    # creates an empty list just to store the top 5 values for the adverse affects to index into later for the ML model
    TopStroke = []
    TopMortality = []
    TopMI = []
    count = 0
    for x in stroke_significant_features_sorted.Feature:
      TopStroke.append(x)
      count = count + 1
      if count == 5:
        break
    count = 0
    for x in mortality_significant_features_sorted.Feature:
      TopMortality.append(x)
      count = count + 1
      if count == 5:
        break
    count = 0
    for x in mi_significant_features_sorted.Feature:
      TopMI.append(x)
      count = count + 1
      if count == 5:
        break

    MortalityY = df['MORTALITY_EVENT']
    MortalityX = df[TopMortality]

    MIY = df['MI_EVENT']
    MIX = df[TopMI]

    StrokeY = df['STROKE_EVENT']
    StrokeX = df[TopStroke]



    weights = {0:1, 1:100}
    MortalityTrainX, MortalityTestX, MortalityTrainY, MortalityTestY = train_test_split(MortalityX, MortalityY, test_size = 0.2, stratify=MortalityY, random_state = 42)
    MITrainX, MITestX, MITrainY, MITestY = train_test_split(MIX, MIY, test_size = 0.2, stratify=MIY, random_state = 42)
    StrokeTrainX, StrokeTestX, StrokeTrainY, StrokeTestY = train_test_split(StrokeX, StrokeY, test_size = 0.2, stratify=StrokeY, random_state = 42)
    Folds = 30

    logisticRegMort = LogisticRegression(class_weight = weights)
    #logisticRegMort = LogisticRegression()
    logisticRegMort.fit(MortalityTrainX, MortalityTrainY)

    def FoldModel(Folds, X, Y, Model):
      allConfusionMatrix = []
      allPrecision = []
      allRecall = []
      #allAccuracy = []
      allF1Score = []
      addConfusionMatrix = [[0,0],[0,0]]

      foldThis = KFold(n_splits = Folds, shuffle = True, random_state = 42)
      crossScore = cross_val_score(Model, X, Y, cv = foldThis)
      print("Average mean accuracy scores", np.mean(crossScore))
      for fold, (train_index, test_index) in enumerate(foldThis.split(X)):
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = Y.iloc[train_index], Y.iloc[test_index]

        Model.fit(X_train, y_train)
        y_pred = Model.predict(X_test)
        tempConfusionMatrix = metrics.confusion_matrix(y_test, y_pred)
        allConfusionMatrix.append(tempConfusionMatrix)
        allF1Score.append(metrics.f1_score(y_test, y_pred, average="weighted"))
        allRecall.append(metrics.recall_score(y_test, y_pred))
        allPrecision.append(metrics.precision_score(y_test, y_pred, average = "weighted"))

      for x in allConfusionMatrix:
        addConfusionMatrix[0][0] = addConfusionMatrix[0][0] + x[0][0]
        addConfusionMatrix[1][0] = addConfusionMatrix[1][0] + x[1][0]
        addConfusionMatrix[0][1] = addConfusionMatrix[0][1] + x[0][1]
        addConfusionMatrix[1][1] = addConfusionMatrix[1][1] + x[1][1]

      TN = addConfusionMatrix[0][0]
      FN = addConfusionMatrix[1][0]
      FP = addConfusionMatrix[0][1]
      TP = addConfusionMatrix[1][1]


      print("TP: " + str(TP))
      print("FP: " + str(FP))
      print("FN: " + str(FN))
      print("TN: " + str(TN))

      #f1_score = metrics.f1_score(y_test, y_pred, average="weighted")
      f1_score = np.mean(allF1Score)
      print("\nf1 score:",f1_score)

      recall = np.mean(allRecall)
      print("Recall:", recall)

      precision =  np.mean(allPrecision)
      print("Precision:", precision)


      #print("\nConfusion matrix\n Left to right then top to bottom: \n True Positive, False Negative, False Positive, True Negative")
      #print(addConfusionMatrix)

    #logisticRegMI = LogisticRegression(class_weight = weights)
    logisticRegMI = LogisticRegression(class_weight = weights)
    logisticRegMI.fit(MIX, MIY)

    weights = {0:1, 1:100}
    logisticRegStroke = LogisticRegression(class_weight = weights)
    logisticRegStroke.fit(StrokeTrainX, StrokeTrainY)

    predictionTestMort = logisticRegMort.predict(MortalityTestX)

    predictionTestMI = logisticRegMI.predict(MITestX)

    predictionTestStroke = logisticRegStroke.predict(StrokeTestX)


    # manually coding TP and TN to make sure it is accurate with boiler code below
    count2 = 0
    analyzeList = []
    for x in range(len(predictionTestStroke)):
      if predictionTestStroke[x] != StrokeTestY.values[x]:
        count2 = count2 + 1
        analyzeList.append(str(predictionTestStroke[x]) + ":" + str(StrokeTestY.values[x]))

    #count2 = 0
    #analyzeList = []
    #for x in range(len(predictionTestStroke)):
    #  analyzeList.append(str(predictionTestStroke[x]) + ":" + str(StrokeTestY.values[x]))







    def testDepend(x, y, modelName):
      y_test = x
      y_pred = y
      accuracy = accuracy_score(y_test, y_pred)
      print("\nAccuracy Equation: (# of matches - 1) / # of samples\n" + "Accuracy:", accuracy)

      MSE = metrics.mean_squared_error(y_test, y_pred)
      #print("\nSmaller number better\nMean Squared Error:", MSE)

      #Outputting Confusion Matrix
      confusion_matrix = metrics.confusion_matrix(y_test, y_pred)
      metrics.ConfusionMatrixDisplay(confusion_matrix)
      print("\nConfusion matrix\n Left to right then top to bottom: \n True Negative, False Positive, False Negative, True Positive")
      print(confusion_matrix)
      countFN = confusion_matrix[1][0]
      countFP = confusion_matrix[0][1]
      countTN = confusion_matrix[0][0]
      countTP = confusion_matrix[1][1]

      f1_score = metrics.f1_score(y_test, y_pred, average="weighted")
      print("\nf1 score:",f1_score)


      recall = metrics.recall_score(y_test, y_pred)
      print("Recall:", recall)

      precision = metrics.precision_score(y_test, y_pred, average = 'weighted')
      print("Precision:", precision)

      roc_auc = metrics.roc_auc_score(y_test, y_pred, average= 'weighted')
      print("\nArea Under the Receiving Operating Characteristic Curve:", roc_auc)

      print("\n\nCALCULATIONS ARE FOR " + modelName)
      print("\n\n")

    def twentysplit():
      testDepend(MITestY, predictionTestMI, "MI(Heart attack)" )

      testDepend(MortalityTestY, predictionTestMort, "Mortality")

      testDepend(StrokeTestY, predictionTestStroke, "Stroke")
    def foldSplit():
      FoldModel(Folds, MortalityX, MortalityY, logisticRegMort)

      FoldModel(Folds, StrokeX, StrokeY, logisticRegStroke)

      FoldModel(Folds, MIX, MIY, logisticRegMI)

    def predict_novel(df2):
      noveldf = df2
      MortalityNovelX = noveldf[TopMortality]
      MortalityNovelY = noveldf['MORTALITY_EVENT']
      MortalityNovelYPred = logisticRegMort.predict(MortalityNovelX)

      StrokeNovelX = noveldf[TopStroke]
      StrokeNovelY = noveldf['STROKE_EVENT']
      StrokeNovelYPred = logisticRegStroke.predict(StrokeNovelX)

      MINovelX = noveldf[TopMI]
      MI_NovelY = noveldf['MI_EVENT']
      MI_NovelYPred = logisticRegMI.predict(MINovelX)

      testDepend(MortalityNovelY, MortalityNovelYPred, "Mortality")

      testDepend(StrokeNovelY, StrokeNovelYPred, "Stroke")

      testDepend(MI_NovelY, MI_NovelYPred, "MI")